---
title: "GDM Threshold considerations"
author: 'Noëlle Schenk'
date: "5/27/2021"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

# Aim

# Procedure

1.  **calculate Threshold averages** : with script
    `cluster_scripts_documentation.Rmd` in order to produce the input
    datasets for this script
2.  Quality check of step 1
3.  calculate threshold averages for maxspline (--> average barplots)
4.  calculate threshold averages for whole ispline (--> average lineplots)

# Produce input data

Please run the procedure described in
`cluster_scripts_documentation.Rmd`, under plot Uncertainty : get sd of
maxspline values. The procedure consists of running the edited function
`gdm::plotUncertainty_slim` on each of the thresholds 0.1-0.9 on the
models for EFturnover and EFnestedness. The output datasets are
"gdm_EFturnover_0.\<?\>\_LUI_uncertainty.Rds" (respectively for all
thresholds, and for EFnestedness as well.)

```{r, eval=F}
# dummy code to test if output can be generated in principle
# test <- plotUncertainty_slim(gdminput, sampleSites = 0.3, bsIters = 5, geo = T, cores = 2)
```

Requirements
```{r}
library(corrplot)
```


# Quality check

Aim of this section : assess the quality of the uncertainty calculation which is
done in `cluster_scripts_documentation.Rmd`. Are the datasets complete?
Do the values make sense?

Read input data

```{r}
# Load requirements from <..>nonpublic.R file
sections_to_be_loaded <- c("thresholds")
# sections_to_be_loaded <- c("thresholds", "results", "gdminput", "gdmoutput")
#TODO : model_results.csv wants to be read, but doesn't exist. Put new name
source("vignettes/analysis_nonpublic.R")
theme_set(theme_half_open())
```

The list `ispline_uncertainty_all_thresholds` contains the deviance of
each ispline separately. Elements of the list are named accordingly.

```{r}
# Quality check of cluster stuff
par(mfrow = c(1, 2))
# put07 <- ispline_uncertainty_all_thresholds[["gdm_EFturnover_0.7"]]
put01 <- ispline_uncertainty_all_thresholds[["gdm_EFturnover_0.1"]]

# plot to see if makes sense

# PLOT 1
# autotroph sim
plot(put01$autotroph.beta.sim_fullModel_X, put01$autotroph.beta.sim_fullModel_Y, 
     type = "l", ylim = c(-0.1, 1))
lines(put01$autotroph.beta.sim_fullModel_X, put01$autotroph.beta.sim_minusSD_Y)
lines(put01$autotroph.beta.sim_fullModel_X, put01$autotroph.beta.sim_plusSD_Y)
# geodist
plot(put01$Geographic_fullModel_X, put01$Geographic_fullModel_Y, 
     type = "l", ylim = c(-0.1, 1))
lines(put01$Geographic_fullModel_X, put01$Geographic_minusSD_Y)
lines(put01$Geographic_fullModel_X, put01$Geographic_plusSD_Y)


# PLOT 2
# LUI
plot(put01$LUI_fullModel_X, put01$LUI_fullModel_Y, type = "l", 
     ylim = range(put01$LUI_fullModel_Y) + c(-0.02, 0.05))
polygon(x = c(put01$LUI_fullModel_X, rev(put01$LUI_fullModel_X)), 
        y = c(put01$LUI_minusSD_Y, rev(put01$LUI_plusSD_Y)), 
        col = "gray", border = NA)
lines(put01$LUI_fullModel_X, put01$LUI_fullModel_Y)
```

X values should be the same everywhere
```{r}
plot(put01$Geographic_fullModel_X, put01$autotroph.beta.sim_fullModel_X)
m <- cor(put01[, grep("fullModel_X", names(put01))], method = "spearman")
corrplot(m, tl.cex = 0.2, type = "lower",
         method = "number", number.cex = 0.3)
```




# Calculate weighted average models

**average effects over all thresholds weighted by ispline sd**

from Gossner 2016: 
- We also calculated a bootstrapped P value for each term in the full GDM, using the gdm.varImp function in the gdm library (Supplementary Table 5-2). Additionally we estimated uncertainty for the GDM plots by using 100 bootstraps for each model, each time removing 30% of the plot pairs and then fitting a GDM and extracting the predictions. We then calculated the s.d. of the predictions and added this (±-) to the fitted line (Extended Data Fig. 6).


runned permutations on the cluster with following parameters :
`plotUncertainty_getsd(gdm_model, sampleSites = 0.3, bsIters = 100, geo = T, splineCol = "black", cores = 2)`
results are stored at :
planteco/.../BetaDivMultifun/analysis/output_datasets/uncertainty_calc/

Calculate weighted average over all models. The GDM models are not all of the same quality. They are weighted based on the uncertainty of each ispline ($w$).

$$wa = \frac{e_{1}*w_{1} + e_{2}*w_{2} + ... + e_{9}*w_{9}}{w_{1}+w_{2} + ... + w_{9}} = \frac{\sum_{i=1}^{n} e_{n}+\frac{1}{sd_{n}}}{\sum_{i=1}^{n} \frac{1}{sd_{n}}} ;  w_{n} = \frac{1}{sd_{n}} $$
where the estimate $e_{n}$ is weighted by its uncertainty $w_{n}$.


## Clean the uncertainty ispline table
For each Predictor : 
- get X values : are the same in all models --> merge by this value
- get Y values : model specific _minusSD_Y, fullModel_Y and _plusSD_Y

```{r, eval = T}
# names(ispline_uncertainty_all_thresholds) # names of the list correspond to model names
# To each element of the list (each data frame), add a column with the respective model name
# use workaround : lapply only gives you the elements of the vector you pass it. 
#    The usual work-around is to pass it the names or indices of the vector instead 
#    of the vector itself.
# The list is assigned to an additional argument of the function. i corresponds to the unassigned
#    function argument, which is the sequ_along().
ispline_uncertainty_all_thresholds <- lapply(seq_along(ispline_uncertainty_all_thresholds), 
                function(y, n, i) {y[[i]]$model_name <- n[i]; y[[i]]$value_ID <- seq(1, 8350); return(y[[i]])},
       y = ispline_uncertainty_all_thresholds,
       n = names(ispline_uncertainty_all_thresholds))
# pile all datasets on top of each other
ispline_uncertainty_all_thresholds <- rbindlist(ispline_uncertainty_all_thresholds, use.names = T)
# filter out the relevant columns : exclude x uncertainties
relevant_column_names <- unique(c("model_name", "value_ID",
                           grep("_fullModel_X|_minusSD_Y|fullModel_Y|_plusSD_Y|SDn",
                                names(ispline_uncertainty_all_thresholds), value = T)
                           ))
ispline_uncertainty_all_thresholds <- ispline_uncertainty_all_thresholds[, ..relevant_column_names]
rm(relevant_column_names)

# filter relevant x values
# 8350 are just too many x values. Is not more correct if I have more values.
# take around 334 values, plus the max value.
relevant_rowindices <- c(seq(1, 8350, by = 25), 8350)
ispline_uncertainty_all_thresholds <- ispline_uncertainty_all_thresholds[value_ID %in% relevant_rowindices, ]
#TODO rename the rownames to 1 : 335 again --> easier to handle!

# # #
# quality control
# plot individual predictor of one model
ggplot(ispline_uncertainty_all_thresholds[model_name == "gdm_EFturnover_0.2",
                                          .(autotroph.beta.sne_fullModel_X, autotroph.beta.sne_fullModel_Y)], 
       aes(x = autotroph.beta.sne_fullModel_X, autotroph.beta.sne_fullModel_Y)) +
  geom_line()
ggplot(ispline_uncertainty_all_thresholds[model_name == "gdm_EFnestedness_0.5", .(autotroph.beta.sne_fullModel_X, autotroph.beta.sne_fullModel_Y)], aes(x = autotroph.beta.sne_fullModel_X, autotroph.beta.sne_fullModel_Y)) +
  geom_line()

# plot individual predictor of all models
ggplot(ispline_uncertainty_all_thresholds, aes(x = autotroph.beta.sne_fullModel_X, autotroph.beta.sne_fullModel_Y, col = model_name)) +
  geom_line()

ggplot(ispline_uncertainty_all_thresholds, 
       aes(x = autotroph.beta.sim_fullModel_X, autotroph.beta.sim_fullModel_Y, 
           col = model_name)) +
  geom_line()

# save output
# saveRDS(ispline_uncertainty_all_thresholds, file = paste0(pathtoout, "/allmod_LUI_uncertainty.Rds"))
```

## Calculate weighted average

Calculate $w$ for each value of X for each predictor for each model individually. In other words, calculate how wide the confidence interval is at a given place in X for a given predictor and model.
Speciality : All models of the same predictor have the same X values, which are identified with value_ID.

Calculate $w$ : $\text{minusSD_Y} = \mu - \sigma$ and $\text{plusSD_Y} = \mu + \sigma$ . Therefore : $\mu + \sigma - (\mu - \sigma) = \mu + \sigma - \mu + \sigma = 2\sigma$ And thus : $w = \sigma = \frac{\text{plusSD_Y} - \text{minusSD_Y}}{2}$

```{r}
# ispline_uncertainty_all_thresholds <- readRDS(paste0(pathtodata, "/analysis/output_datasets/uncertainty_calc/allmod_LUI_uncertainty.Rds"))
ispline_uncertainty_all_thresholds <- melt(ispline_uncertainty_all_thresholds, id.vars = c("model_name", "value_ID"))
ispline_uncertainty_all_thresholds[, predictor := sub("_[a-zA-Z]+_[XY]$", "", variable, perl = T)]
ispline_uncertainty_all_thresholds[, predictor := sub("_SDn", "", predictor)]
ispline_uncertainty_all_thresholds[, Variable_type := sub("soil_|isolation_", "", sub("^[a-zA-Z.]+_", "", variable, perl = T), perl = T)] 
ispline_uncertainty_all_thresholds[, variable := NULL]
ispline_uncertainty_all_thresholds[grep("X", ispline_uncertainty_all_thresholds$Variable_type), XY := "X"]
ispline_uncertainty_all_thresholds[grep("Y", ispline_uncertainty_all_thresholds$Variable_type), XY := "Y"]
ispline_uncertainty_all_thresholds[, Model_type := "nestedness"]
ispline_uncertainty_all_thresholds[grep("turnover", model_name), Model_type := "turnover"]

# 1. create dataset with X values which will be used to get the rest
Xtest <- ispline_uncertainty_all_thresholds[XY == "X", ]
setnames(Xtest, old = "value", new = "Xvalue")
Xtest <- Xtest[, .(model_name, Model_type, predictor, Xvalue, value_ID)]

# 2. merge the Y values to this one
ispline_uncertainty_all_thresholds <- merge(Xtest, ispline_uncertainty_all_thresholds[XY == "Y", .(model_name, Model_type, predictor, value_ID, value, Variable_type)],
      by = c("model_name", "predictor", "value_ID", "Model_type"))
rm(Xtest)

ispline_uncertainty_all_thresholds <- dcast(ispline_uncertainty_all_thresholds, model_name + Model_type + predictor + value_ID + Xvalue ~ Variable_type)

# # #
# Calculate the weight SD_Y
#
# (1) calculate uncertainty
ispline_uncertainty_all_thresholds[, weight := (plusSD_Y - minusSD_Y)/2]
range(ispline_uncertainty_all_thresholds$weight)
hist(ispline_uncertainty_all_thresholds$weight, xlim = c(0, 0.95))
hist(ispline_uncertainty_all_thresholds$weight, xlim = c(0, 0.95), ylim = c(0, 4000))
# (2) handling 0 cases
#   the standard deviation is 0 in some cases (where all predictions are 0)
#   we could replace the 0 and very small values with the second smallest value
hist(ispline_uncertainty_all_thresholds$weight, xlim = c(0, 0.000001), ylim = c(0, 4000))
min(ispline_uncertainty_all_thresholds$weight[!(ispline_uncertainty_all_thresholds$weight == 0)]) 
# is way larger than the machine epsilon (e-12 vs e-16).
# we could also use a 3-step approach : 
#     a. normalise standard deviations to values between 0 and 1
#     b. calculate the weights 1 / sd
#        and replace the Inf values with 1 (maximum weight)
#     c. normalise again
# but also here, at one point we have to replace 0 or Inf values with 
#   a number. Therefore, we go for the approach where 0 are replaced by epsilon.
# ispline_uncertainty_all_thresholds[weight == 0, weight := .Machine$double.eps]
ispline_uncertainty_all_thresholds[weight == 0, weight := min(ispline_uncertainty_all_thresholds$weight[!(ispline_uncertainty_all_thresholds$weight == 0)])]
#
# (3) calculate weight
ispline_uncertainty_all_thresholds[, weight := 1 / weight]
hist(ispline_uncertainty_all_thresholds$weight)
# (4) normalise to weights between 0 and 1
#  the normalisation needs to be done per value_ID, because small values
#    have smaller standard deviations. the models should get weights according
#    to the X value. There should not be any dependency between the weith of x = 0.2 and
#    x = 0.3.
# normalise weights between 0 and 1
#   per predictor
#   per valueID
#   per model type (turnover vs. nestedness)
#     each normalisation is a normalisation of 9 values
nrow(ispline_uncertainty_all_thresholds) == 18 * 35 * 335

# do the normalisation NOT over all, but grouped by
# note : no shifting to zero needed, because values are already shifted to 0.
# found this nice data.table code, need to test if does what I want.
setDT(ispline_uncertainty_all_thresholds)[, Nor := weight / max(weight, na.rm = T), 
          by = list(predictor, value_ID, Model_type)]
# leads to NaN values if weight is the minimum --> 
ispline_uncertainty_all_thresholds[predictor == "Geographic" & value_ID == 1 & Model_type == "nestedness", ] # replace NaN with 1
ispline_uncertainty_all_thresholds[predictor == "Geographic" & value_ID == 26 & Model_type == "nestedness", ]
hist(ispline_uncertainty_all_thresholds[predictor == "Geographic" & value_ID == 1 & Model_type == "nestedness", Nor])
# fuck, so bechömä eifach d 0 wärtä viiu meh gwicht!
#TODO schauen ob das ein Problem ist. --> ds isch wenigstens immer glich

# clean
ispline_uncertainty_all_thresholds[, weight := Nor]
ispline_uncertainty_all_thresholds[, Nor := NULL]


# calculate the weighted mean of fullModel_Y
ispline_uncertainty_all_thresholds[, weighted_Y := fullModel_Y * weight]
setDT(ispline_uncertainty_all_thresholds)[, sum_of_weight := sum(weight), by = list(predictor, value_ID, Model_type)]
setDT(ispline_uncertainty_all_thresholds)[, sum_of_weighted_Y := sum(weighted_Y), by = list(predictor, value_ID, Model_type)]
ispline_uncertainty_all_thresholds[, mean_Y := sum_of_weighted_Y/ sum_of_weight]
ispline_uncertainty_all_thresholds[, c("sum_of_weight", "sum_of_weighted_Y", "weighted_Y") := NULL]

# save output dataset
# saveRDS(ispline_uncertainty_all_thresholds, file = paste0(pathtoout, "/allmod_LUI_uncertainty_weighted_avg.Rds"))
```

## Plotting
```{r}
# ispline_uncertainty_all_thresholds <- readRDS(paste0(pathtodata, "/analysis/output_datasets/uncertainty_calc/allmod_LUI_uncertainty_weighted_avg.Rds"))

# Quality checks
ggplot(ispline_uncertainty_all_thresholds[predictor == "autotroph.beta.sne" & Model_type == "nestedness"], 
       aes(x = Xvalue, y = fullModel_Y, col = model_name, fill = model_name)) +
  geom_line() +
  geom_ribbon(aes(ymin = minusSD_Y, ymax = plusSD_Y), alpha = 0.1, linetype = 0) +
  scale_fill_brewer(palette="Set1") +
  scale_color_brewer(palette="Set1") +
  coord_cartesian(ylim = c(-0.02, 0.1)) + xlab("") + ylab("") +
  geom_line(aes(x = Xvalue, y = mean_Y), linewidth = 1.5, color = "red")

ggplot(ispline_uncertainty_all_thresholds[predictor == "LUI" & Model_type == "nestedness"], 
       aes(x = Xvalue, y = fullModel_Y, col = model_name, fill = model_name)) +
  geom_line() +
  scale_fill_brewer(palette="Set1") +
  scale_color_brewer(palette="Set1") +
  coord_cartesian(ylim = c(-0.02, 0.1)) + xlab("") + ylab("") +
  geom_line(aes(x = Xvalue, y = mean_Y), linewidth = 1.5, color = "red")
#
ggplot(ispline_uncertainty_all_thresholds[predictor == "plot_isolation" & Model_type == "nestedness",], 
       aes(x = Xvalue, y = fullModel_Y, col = model_name, fill = model_name)) +
  geom_line() +
  scale_fill_brewer(palette="Set1") +
  scale_color_brewer(palette="Set1") +
  coord_cartesian(ylim = c(-0.02, 0.1)) +
  geom_line(aes(x = Xvalue, y = mean_Y), linewidth = 1.5, color = "red") +
  xlab("") + ylab("")
#
```

Structured plotting

Match with plotting table `nicenames`
```{r}
setnames(ispline_uncertainty_all_thresholds, old = "predictor", new = "names")
# reshape table : bring mean to it's own row
test <- melt(ispline_uncertainty_all_thresholds,
     measure.vars = c("fullModel_Y", "mean_Y"),
     id.vars = c("model_name", "Model_type", "value_ID", "names", "Xvalue")
    )
test[variable == "mean_Y", model_name := paste0("weighted_mean_", Model_type)]
test[, variable := NULL]
test <- unique(test)
nrow(test)  == nrow(ispline_uncertainty_all_thresholds) + 335 * 35 * 2
# has 335 * 35 * 2 rows more. Should contain 234500 rows.
# add again the Yvalues from ispline_uncertainty
test <- merge(test, ispline_uncertainty_all_thresholds, 
      by = c("model_name", "Model_type", "value_ID", "Xvalue", "names"),
      all.x = T)
test[, c("fullModel_Y", "weight", "mean_Y") := NULL]

ispline_uncertainty_all_thresholds <- merge(test, nicenames, by = c("names"))

setnames(ispline_uncertainty_all_thresholds, 
         old = c("Xvalue"),
         new = c("xaxis"))
```

Plot each predictor individually, showing all model lines
```{r}
type <- "nestedness"
p <- "LUI"
for(type in c("nestedness", "turnover")){
  for(p in unique(ispline_uncertainty_all_thresholds$names)){
    
  }
}


```



```{r}
create_gdm_lineplot(ispline_uncertainty_all_thresholds[model_name == "weighted_mean_turnover" & ground == "a", ],
                        ymax = 0.05)
create_gdm_lineplot(ispline_uncertainty_all_thresholds[model_name == "weighted_mean_turnover" & ground == "b", ],
                        ymax = 0.05)
create_gdm_lineplot(ispline_uncertainty_all_thresholds[model_name == "weighted_mean_turnover" & type == "abio", ],
                        ymax = 0.05)

```






















```{r}
# Quality check
#    X values of different thresholds of same predictor should be the same
#
# secondary consumers nestedness
qc_dat <- dcast(ispline_uncertainty_all_thresholds[predictor == "secondary.consumer.beta.sne" & model_name %in% c("gdm_EFnestedness_0.1", "gdm_EFnestedness_0.2", "gdm_EFnestedness_0.3"), .(value_ID, Xvalue, model_name)],
      value_ID ~ model_name, value.var = "Xvalue")
qc_dat[, qc := gdm_EFnestedness_0.1 - gdm_EFnestedness_0.2 + 
         gdm_EFnestedness_0.3 - gdm_EFnestedness_0.1]
sum(qc_dat$qc != 0) == 0 # all xvalues are the same
#
# Geographic distance
qc_dat <- dcast(ispline_uncertainty_all_thresholds[predictor == "Geographic" & model_name %in% c("gdm_EFnestedness_0.1", "gdm_EFnestedness_0.2", "gdm_EFnestedness_0.3"), .(value_ID, Xvalue, model_name)],
      value_ID ~ model_name, value.var = "Xvalue")
qc_dat[, qc := gdm_EFnestedness_0.1 - gdm_EFnestedness_0.2 + 
         gdm_EFnestedness_0.3 - gdm_EFnestedness_0.1]
sum(qc_dat$qc != 0) == 0 # all xvalues are the same
#
# manually checked for 4 further predictors (all types)
```



Calculation of 

```{r}
#TODO HERE : mission accomplished! reformated huge table.
#    next steps : 
#        - [DONE] calculate $w$ = SD, calculate from minusSD_Y and plusSDY.
#        - [DONE] include the missing two models turnover 0.8 and 0.9 --> calc by hand!
#        - aggregate the mean (fullModel_Y) weighted by SD over all thresholds.

# aggregate mean over all thresholds : 
# for each predictor individually : 




# note : old code below, no idea if needed
stats::aggregate(fullModel_Y ~ predictor + Model_type + value_ID, 
                 FUN = mean,
                 data = ispline_uncertainty_all_thresholds)

# BELOW : testing around
# 
# dcast(test, id.vars = c("predictor", "model_name"))
# dcast(test, predictor + model_name ~ Variable_type + XY, value.var = "value")
# #fullModel_X needs to be a new column
# 
# relevant_column_names <- relevant_column_names <- grep("_fullModel_X", names(ispline_uncertainty_all_thresholds), value = T, invert = T)
# test2 <- melt(ispline_uncertainty_all_thresholds, id.vars = c(relevant_column_names)) #measure.vars = relevant_column_names)
# # # quality check
# # test3 <- test2[, .(model_name, variable, value, autotroph.beta.sim_fullModel_Y)]
# # setorder(test3, cols = "value")
# # test3
# setnames(test2, old = "variable", new = "predictorX")
# test2
# 
# 
# # # Bring dataset to long format
# # ispline_uncertainty_all_thresholds <- melt(ispline_uncertainty_all_thresholds, id.vars = "model_name")
# # # assign additional variable "model_class" to distinguish turnover and nestedness
# # ispline_uncertainty_all_thresholds[grep("turnover", ispline_uncertainty_all_thresholds$model_name), 
# #                                    model_class := "turnover"]
# # ispline_uncertainty_all_thresholds[grep("nestedness", ispline_uncertainty_all_thresholds$model_name), model_class := "nestedness"]
# #             
```




```{r}
#unsure : aggregating only the max value ID, because I want to have only the
# barplots for the PES presentation 26.04.23
test <- ispline_uncertainty_all_thresholds[value_ID == 200, ]

# weigh the effect size and average to get summary over all thresholds!
# (1) get the weighted effect size for each effect
test[, weighted_effect := fullModel_Y * weight]
# (2) aggregate two things : 
#    (2.1.) aggregate the weighted effect across all thresholds
#    (2.2.) aggregate the weigh across all thresholds
test1 <- aggregate(weighted_effect ~ predictor + Model_type, FUN = sum, na.rm = T, data = test)
test2 <- aggregate(weight ~ predictor + Model_type, FUN = sum, na.rm = T, data = test)
test3 <- merge(test1, test2, by = c("predictor", "Model_type"))
rm(test1, test2)
test3$weighted_avg_effect <- test3$weighted_effect / test3$weight
test3 <- as.data.table(test3)


# plot
ggplot(test3[Model_type == "turnover", ], aes(x = predictor, y = weighted_avg_effect)) +
  geom_bar(stat = "identity") + 
  coord_flip()
```









# ...
# ...

# DEPRECIATED

requirements : - TODO user : connect to planteco and nsch to get input
files - GDM output as .rds, e.g. gdm_EFnestedness_LUI_input.Rds - find
at planteco/.../BetaDivMultifun/analysis/output_datasets - cluster
output as .rds, e.g. gdm_EFnestedness_LUI_permutation.Rds - run
`results_nonpublic.R` : load pathtodata, nicenames and model_names -
pathtodata : path to location where gdm input of model is stored. points
to a folder, where the folder cluster witht the model input is. *note :
was pathtoout before.* - nicenames : helper file to get nice names of
functions and trophic levels - model_names : helper file for automation.
Contains 3 columns : `name` contains the all model names for
`model_name`, `luiinput` contains either "lui" or "components" as LUI
input for `luiinput` and `permut` is either TRUE or FALSE. -
`analysis_nonpublic.R` loads `EFmaster_all_thresholds.Rds`

# TODO HERE




# thresholds EF heatmap

```{r, eval = F}
# dataset is stored, next chunk reads it in
mod_names <- model_results[EF %in% c("distance", "nestedness", "turnover") & luiinput == "lui", name]
mod_names <- mod_names[-grep("0.75", mod_names)]

i <- mod_names[1]
a <- paste(pathtodata, "/analysis/output_datasets/", i, "_output.Rds", sep = "")
gdm_output <- readRDS(a)
# get maxsplines
exSplines <- gdm::isplineExtract(gdm_output)
# take maximum of each variable for barplot
maxsplines <- apply(exSplines$y, 2, max, na.rm = TRUE)
restab <- data.table(model = i, data.frame(as.list(maxsplines)))

for(i in mod_names[-1]){
  a <- paste(pathtodata, "/analysis/output_datasets/", i, "_output.Rds", sep = "")
  gdm_output <- readRDS(a)
  # get maxsplines
  exSplines <- gdm::isplineExtract(gdm_output)
  # take maximum of each variable for barplot
  maxsplines <- apply(exSplines$y, 2, max, na.rm = TRUE)
  restab_newline <- data.table(model = i, data.frame(as.list(maxsplines)))
  # append to restab
  restab <- rbindlist(list(restab, restab_newline), use.names = T)
  rm(restab_newline); rm(gdm_output)
}
rm(i); rm(exSplines); rm(a); rm(maxsplines)

# make restab column names nice
setnames(restab, old = as.character(nicenames$names),  new = as.character(nicenames$nicenames), skip_absent = T)
# transpose data.table
restab <- dcast(melt(restab, id.vars = "model"), variable ~ model)
setnames(restab, old = "variable", new = "nicenames")
restab <- merge(nicenames, restab, by = "nicenames")

# save
saveRDS(restab, file = "vignettes/out/plot_GDM_results_EFthresholds.rds")
```

```{r}
# Load data :
restab <- readRDS(paste(pathtodata, "/analysis/output_datasets/plot_GDM_results_EFthresholds.rds", sep = ""))
# need to add component column
restab <- merge(restab, nicenames[, .(names, nicenames, ground, component)], by = c("names", "nicenames", "ground"))
fwrite(restab, file = paste(pathtoout, "/all_LUI_mods_results_overview.csv", sep = ""), sep = ",") # write for supplementary
```

create heatmap

```{r, eval = F}
include <- c("nicenames", names(restab)[grep("gdm", names(restab))])
# pal <- rev(RColorBrewer::brewer.pal(9, "RdYlBu"))
pal <- RColorBrewer::brewer.pal(9, "YlOrRd")
# pal <- c("white", RColorBrewer::brewer.pal(9, "YlOrRd"))[-2]
# colgrad <- melt(restab2[, ..include], id.vars = c("nicenames", "n_affected_fun"))
# colgrad <- colgrad$value

df <- restab[ground == "a", ]
df <- data.table::melt(df[, ..include], id.vars = c("nicenames"))
df[value == 0, value := NA] # 0 values are set to NA to make them appear grey in the plot

a <- ggplot(data = df, aes(x = variable, y = nicenames, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colours = pal, na.value = "grey80") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, 
    size = 9, hjust = 1),
    plot.margin = margin(l = 50, r = 20),
    axis.line.x=element_blank(),
    axis.text.y = element_text(size=9, angle = 0),
    axis.ticks.x = element_blank(),
    panel.grid.major.x = element_line(color = "grey"),
    axis.title = element_blank(),
    legend.position = "top") +
  scale_x_discrete(position = "top")

df <- restab[ground == "b", ]
df <- data.table::melt(df[, ..include], id.vars = c("nicenames"))
df[value == 0, value := NA]
b <- ggplot(data = df, aes(x = variable, y = nicenames, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colours = pal, na.value = "grey80") +
  theme(legend.position = "none", 
        axis.title = element_blank(),
        axis.text.y = element_text(size=9, angle = 0),
        plot.margin = margin(l = 50, r = 20),
        axis.text.x = element_blank(),
        axis.line.x=element_blank(),
        axis.ticks.x = element_blank(),
        panel.grid.major.x = element_line(color = "grey"))


df <- restab[ground == "x", ]
df <- data.table::melt(df[, ..include], id.vars = c("nicenames"))
df[value == 0, value := NA]
c <- ggplot(data = df, aes(x = variable, y = nicenames, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colours = pal, na.value = "grey80") +
  theme(legend.position = "none", 
        axis.title = element_blank(),
        axis.text.y = element_text(size=9, angle = 0),
        plot.margin = margin(l = 50, r = 20),
        axis.text.x = element_blank(),
        axis.line.x=element_blank(),
        axis.ticks.x = element_blank(),
        panel.grid.major.x = element_line(color = "grey"))


heatmap <- plot_grid(a, b, c, labels = c('A', '', 'B'), label_size = 12, 
                     nrow = 3, rel_heights = c(0.52, 0.39, 0.09), align = "v")

ggsave(filename = paste(pathtoout, "/plot_GDM_thresholds_heatmap.pdf", sep = ""), plot = heatmap, device=cairo_pdf,
       width = 11.96, height = 8.27, units = "in", dpi = 900)
```

clustered heatmap

```{r, eval = F}
restab <- readRDS(paste(pathtodata, "/analysis/output_datasets/plot_GDM_results_EFthresholds.rds", sep = ""))

include <- c("nicenames", names(restab)[grep("gdm", names(restab))])
include <- include[-which(include %in% c("gdm_EFdistance_LUI", "gdm_EFturnover_median_LUI", "gdm_EFnestedness_median_LUI"))]

restab1 <- restab[, ..include]
restab1 <- as.matrix(restab1[, -1])
rownames(restab1) <- restab$nicenames
colnames(restab1) <- gsub("EFnestedness_", "NES ", gsub("EFturnover_", "TO ", 
                                                        gsub("gdm_", "", gsub("_LUI", "", colnames(restab1)))))

heatmap(restab1, hclustfun = hclust)
# manually saved as "plot_GDM_thresholds_heatmap_cluster.pdf
```

Create overviewbars

```{r, eval = F}
# Load data :
restab <- readRDS(paste(pathtodata, "/analysis/output_datasets/plot_GDM_results_EFthresholds.rds", sep = ""))
# need to add component column
restab <- merge(restab, nicenames[, .(names, nicenames, ground, component)], by = c("names", "nicenames", "ground"))
# select the relevant columns
include <- grep("median", names(restab), value = T, invert = T)
restab <- restab[, ..include]
rm(include)

a <- create_single_funs_overviewbars(restab2 = restab, 
                                     rel_colnames = grep("gdm_", names(restab), value = T))
# bring to ggplot format
ov_ab_allthresholds <- melt(a$above_below, id.vars = c("ground", "color"))
ov_tn_allthresholds <- melt(a$turnover_nestedess, id.vars = c("component", "color"))
# create plots
p_ov_ab_thres <- create_single_funs_overviewbar_plot(singleF_restab = ov_ab_allthresholds, pos = "dodge")
p_ov_tn_thres <- create_single_funs_overviewbar_plot(singleF_restab = ov_tn_allthresholds, pos = "dodge")

p_ov_abtn_thres <- plot_grid(p_ov_ab_thres, p_ov_tn_thres, labels = c("A", "B"))
# saved as : <date>_GDM_multifun_thresholds_allthresholds_overview_bars<_dodge>
```

# calc average effects over all thresholds

```{r, eval = F}
# created dataset which is read in in next chunk
restab <- readRDS(paste(pathtodata, "/analysis/output_datasets/plot_GDM_results_EFthresholds.rds", sep = ""))
include <- c("nicenames", names(restab)[grep("gdm", names(restab))])
include <- include[-which(include %in% c("gdm_EFdistance_LUI", "gdm_EFturnover_median_LUI", "gdm_EFnestedness_median_LUI"))]
restab <- restab[, ..include]
# long format for aggregate
restab1 <- melt(restab, id.vars = "nicenames")
restab1[grep("turnover", variable), component := "EFturnover"]
restab1[grep("nestedness", variable), component := "EFnestedness"]

# calculate average and sd
EFmean <- data.table(aggregate(value ~ component + nicenames, restab1, function(x) mean(x, na.rm = T)))
setnames(EFmean, old = "value", new = "mean")
# EFsd <- data.table(aggregate(value ~ component + nicenames, restab1, sd))
EFsd <- data.table(aggregate(value ~ component + nicenames, restab1, function(x) sd(x)/sqrt(length(x))))
setnames(EFsd, old = "value", new = "sd")

# combine to new data table
EFmean <- merge(EFmean, EFsd, by = c("component", "nicenames"))
rm(EFsd)
t1 <- EFmean[component == "EFturnover", -"component"]
setnames(t1, old = c("mean", "sd"), new = c("EFturnover_mean", "EFturnover_sd"))
t2 <- EFmean[component == "EFnestedness", -"component"]
setnames(t2, old = c("mean", "sd"), new = c("EFnestedness_mean", "EFnestedness_sd"))
EFmean <- merge(t1, t2, by = "nicenames")
rm(t1); rm(t2)

# add to threshold dataset
restab <- merge(restab, EFmean, by = "nicenames")
# save dataset
saveRDS(restab, file = "vignettes/out/plot_GDM_results_EFthresholds_mean_se.rds")
# note : there is also "vignettes/out/plot_GDM_results_EFthresholds_mean_sd.rds" with standard deviation
```

## Heatmap

```{r, eval = F}
restab <- readRDS(paste(pathtodata, "/analysis/output_datasets/plot_GDM_results_EFthresholds_mean_se.rds", sep = ""))
restab <- merge(nicenames[, .(nicenames, ground)], restab, by = "nicenames")
# OPTION : select the variables to include in heatmap
#    OPTION 2 : take out "white" from color pal
include <- colnames(restab)[-which(colnames(restab) == "ground")]
# include <- c("nicenames", "EFturnover_mean", "EFturnover_sd", "EFnestedness_mean", "EFnestedness_sd")

# pal <- c("white", RColorBrewer::brewer.pal(9, "YlOrRd"))
pal <- c(RColorBrewer::brewer.pal(9, "YlOrRd"))

# below : code copied from heatmap above -----------------------------
df <- restab[ground == "a", ]
df <- data.table::melt(df[, ..include], id.vars = c("nicenames"))
df[value == 0, value := NA] # 0 values are set to NA to make them appear grey in the plot

a <- ggplot(data = df, aes(x = variable, y = nicenames, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colours = pal, na.value = "grey80") +
  theme(axis.text.x = element_text(angle = 90, vjust = 1, 
    size = 9, hjust = 1),
    plot.margin = margin(l = 50, r = 20),
    axis.line.x=element_blank(),
    axis.text.y = element_text(size=9, angle = 0),
    axis.ticks.x = element_blank(),
    panel.grid.major.x = element_line(color = "grey"),
    axis.title = element_blank(),
    legend.position = "top") +
  scale_x_discrete(position = "top")

df <- restab[ground == "b", ]
df <- data.table::melt(df[, ..include], id.vars = c("nicenames"))
df[value == 0, value := NA]
b <- ggplot(data = df, aes(x = variable, y = nicenames, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colours = pal, na.value = "grey80") +
  theme(legend.position = "none", 
        axis.title = element_blank(),
        axis.text.y = element_text(size=9, angle = 0),
        plot.margin = margin(l = 50, r = 20),
        axis.text.x = element_blank(),
        axis.line.x=element_blank(),
        axis.ticks.x = element_blank(),
        panel.grid.major.x = element_line(color = "grey"))


df <- restab[ground == "x", ]
df <- data.table::melt(df[, ..include], id.vars = c("nicenames"))
df[value == 0, value := NA]
c <- ggplot(data = df, aes(x = variable, y = nicenames, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colours = pal, na.value = "grey80") +
  theme(legend.position = "none", 
        axis.title = element_blank(),
        axis.text.y = element_text(size=9, angle = 0),
        plot.margin = margin(l = 50, r = 20),
        axis.text.x = element_blank(),
        axis.line.x=element_blank(),
        axis.ticks.x = element_blank(),
        panel.grid.major.x = element_line(color = "grey"))
# above : code copied from heatmap above -----------------------------

heatmap <- plot_grid(a, b, c, labels = c('A', '', 'B'), label_size = 12, 
                     nrow = 3, rel_heights = c(0.48, 0.42, 0.1), align = "v")

# manually saved small heatmap with only mean and sd (no other thesholds) 
#    as : GDM_thresholds_heatmap_mean_sd_only
ggsave(filename = paste(pathtoout, "/plot_GDM_thresholds_heatmap_mean_se.pdf", sep = ""), plot = heatmap, device=cairo_pdf,
       width = 11.96, height = 8.27, units = "in", dpi = 900)
```

## average effects over all thresholds weighted by model sd

from Gossner 2016 We also calculated a bootstrapped P value for each
term in the full GDM, using the gdm.varImp function in the gdm library
(Supplementary Table 5-2). Additionally we estimated uncertainty for the
GDM plots by using 100 bootstraps for each model, each time removing 30%
of the plot pairs and then fitting a GDM and extracting the predictions.
We then calculated the s.d. of the predictions and added this (±-) to
the fitted line (Extended Data Fig. 6).

runned permutations on the cluster with following parameters :
`plotUncertainty_getsd(gdm_model, sampleSites = 0.3, bsIters = 100, geo = T, splineCol = "black", cores = 2)`
results are stored at :
planteco/.../BetaDivMultifun/analysis/output_datasets/uncertainty_calc

reading in data

```{r}
# Note : go through code and either chose to work with EFnestedness, or EFturnover. (by outcommenting respective lines)

modnames <- paste(paste("gdm_EFturnover_", seq(0.1, 0.9, 0.1), sep = ""), "_LUI", sep = "")
# modnames <- paste(paste("gdm_EFnestedness_", seq(0.1, 0.9, 0.1), sep = ""), "_LUI", sep = "")
i <- 1
res <- readRDS(paste(paste(paste(pathtodata, "/analysis/output_datasets/uncertainty_calc/", sep = ""), modnames[i], sep = ""), "_uncertainty.Rds", sep = ""))
res[, component := modnames[i]]

for(i in 2:9){
  t <- readRDS(paste(paste(paste(pathtodata, "/analysis/output_datasets/uncertainty_calc/", sep = ""), modnames[i], sep = ""), "_uncertainty.Rds", sep = ""))
  t[, component := modnames[i]]
  res <- rbindlist(list(res, t))
}
```

plotting

```{r, eval = F}
a <- ggplot(res[varname %in% nicenames[ground == "a", names]], aes(x = varname, y = yatmax, fill = component)) +
  geom_bar(stat = "identity", position="dodge") +
  coord_flip() +
  geom_errorbar(aes(ymin = yatmax - sdy, ymax = yatmax + sdy), 
                  linetype = "solid",
                position=position_dodge(width=0.9)) +
  scale_fill_brewer(palette="Set3")
b1 <- ggplot(res[varname %in% nicenames[ground == "b", names][1:10]], aes(x = varname, y = yatmax, fill = component)) +
  geom_bar(stat = "identity", position="dodge") +
  coord_flip() +
  geom_errorbar(aes(ymin = yatmax - sdy, ymax = yatmax + sdy), 
                  linetype = "solid",
                position=position_dodge(width=0.9)) +
  scale_fill_brewer(palette="Set3")
b2 <- ggplot(res[varname %in% nicenames[ground == "b", names][11:20]], aes(x = varname, y = yatmax, fill = component)) +
  geom_bar(stat = "identity", position="dodge") +
  coord_flip() +
  geom_errorbar(aes(ymin = yatmax - sdy, ymax = yatmax + sdy), 
                  linetype = "solid",
                position=position_dodge(width=0.9)) +
  scale_fill_brewer(palette="Set3")
x <- ggplot(res[varname %in% nicenames[ground == "x", names]], aes(x = varname, y = yatmax, fill = component)) +
  geom_bar(stat = "identity", position="dodge") +
  coord_flip() +
  geom_errorbar(aes(ymin = yatmax - sdy, ymax = yatmax + sdy), 
                  linetype = "solid",
                position=position_dodge(width=0.9)) +
  scale_fill_brewer(palette="Set3")
# saved by hand and combined to 1 pdf with the name :
#  "GDM_multifun_thresholds_weighted_avg_over_all_thresholds_plotUncertainty_test_intermediate.pdf"
```

Calculate weighted average
$$wa = \frac{e_{1}*w_{1} + e_{2}*w_{2} + ... + e_{9}*w_{9}}{w_{1}+w_{2} + ... + w_{9}} = \frac{\sum_{i=1}^{n} e_{n}+\frac{1}{sd_{n}}}{\sum_{i=1}^{n} \frac{1}{sd_{n}}} ;  w_{n} = \frac{1}{sd_{n}} $$
uncertainty-weighted average effect size.

```{r, eval = F}
######
# calculate WEIGHTED AVERAGE
res[, weights := sdy ^ (-1)]
# if sdy is zero, set weight to 0
res[sdy == 0, weights := 0]
res[, weighted_yatmax := yatmax * weights]
# every variable has its own weight - calculate sum of weights for each variable:
sum_of_weights <- aggregate(weights ~ varname, res, sum)

wres <- data.table(aggregate(weighted_yatmax ~ varname, res, sum))

wres <- merge(wres, sum_of_weights, by = "varname")
wres[, wres := weighted_yatmax / weights]

# test one variable and check results
i <-  "bacterivore.protist.beta.sim"
sum(res[varname == i, yatmax] * (1 / res[varname == i, sdy])) / sum((1 / res[varname == i, sdy]))
# True for Geographic, autotroph.beta.sim, bacterivore.protist.beta.sim

# merge with nicenames to get stuff for barplots
setnames(wres, old = "varname", new = "names")
wres <- merge(wres[, .(names, wres)], nicenames, by = "names")

setnames(wres, old = c("wres"), new = c("maxsplines")) #TODO this line was added to get cleaner input
# saveRDS(wres, file = "vignettes/out/weighted_avg_over_all_thresholds_EFturnover.Rds")
# saveRDS(wres, file = "vignettes/out/weighted_avg_over_all_thresholds_EFnestedness.Rds")
```

prepare for plotting

```{r}
# restab <- readRDS(paste(pathtodata, "/analysis/output_datasets/weighted_avg_over_all_thresholds_EFturnover.Rds", sep = ""))
restab <- readRDS(paste(pathtodata, "/analysis/output_datasets/weighted_avg_over_all_thresholds_EFnestedness.Rds", sep = ""))
setnames(restab, old = c("wres"), new = c("maxsplines")) #TODO : this line is maybe not needed any more
# model_name <- "EFturnover_WEIGTHED_mean_across_thresholds"
model_name <- "EFnestedness_WEIGTHED_mean_across_thresholds"
luiinput <- "lui"
permut <- F
model_names_selection <- data.frame(model_name, luiinput, permut)

# ----------------------------- change for presentation at INTECOL
#TODO HACK to have stacked bars : 
# NEEDS TO BE CLEANED URGENTLY!!!
# instead of plotting with aes(x = nicenames) we use : aes(x = legendnames)
# hack to use original code : rename the according column in the data.table restab
# setnames(restab, old = c("nicenames", "legendnames"), new = c("former_nicenames", "nicenames"))
p <- create_bio_aboveground_barplot(type = "stacked") + ylim(c(0, 0.15))# type is  in c("stacked", "grouped")
b <- create_bio_belowground_barplot(type = "stacked")+ ylim(c(0, 0.15))
q <- create_abio_barplot(type = "stacked") + ylim(c(0, 0.15))# here it does not matter if "stacked" or "grouped" is given.
# ----------------------------- change for presentation at PES End
# creat lui_component to make overviewbars
restab[, lui_component := component]
restab[lui_component == "abio" & names %in% c("deltaLUI", "LUI", "plot_isolation"), lui_component := "LUI"]
restab[lui_component == "abio", lui_component := "abiotic"]
restab[, lui_ground := ground]
restab[lui_ground == "x" & lui_component == "lui", lui_ground := "lui"]
restab[lui_ground == "x", lui_ground_nicenames := "abiotic"]
restab[lui_ground == "a", lui_ground_nicenames := "aboveground"]
restab[lui_ground == "b", lui_ground_nicenames := "belowground"]
restab[lui_ground == "lui", lui_ground_nicenames := "LUI"]
# -----------------------------
# ----------------------------- change for presentation at PES



p <- create_bio_aboveground_barplot() + ylim(c(0, 0.15))
b <- create_bio_belowground_barplot() + ylim(c(0, 0.15))
q <- create_abio_barplot() + ylim(c(0, 0.15))


# go to plot_GDM, and run the part under "# Barplots ". Start with section "produce overview bars" (leave out the upper part)
# reset the ylims : for EFnestedness : + ylim(c(0, 0.1)), for EFturnover : + ylim(c(0, 0.15))
#TODO ERROR plotting does not work any more --> make sure it can be plotted normally with plot_GDM. At the moment, create_restab2() is not working.


# Additional : loaded both EFturnover and EFnestedness, merged together and saved as : 
# "EFnestedness_turnover_WEIGTHED_mean_across_thresholds_maxsplines.csv"


#TODO URGENTLY CLEAN THAT MESS!

multipanel_with_overview_bars <- plot_grid(p, b, q, labels = c('A', 'B', 'C'), label_size = 12, nrow = 3,
                                             rel_heights = c(0.31, 0.49, 0.33), align = "v")
multipanel_with_overview_bars
test <- plot_grid(ov, ovL, nrow = 2)
# saved as : "EFturnover_WEIGHTED_mean_across_tresholds_multipanel_barplot_intecol_presentation_22-08-25.pdf"
# saved as : "EFnestedness_WEIGHTED_mean_across_tresholds_multipanel_barplot_PES_presentation_23-04-26.pdf"
# saved as : "EFnestedness_WEIGHTED_mean_across_tresholds_multipanel_barplot_PES_presentation_23-04-26_overviewbars.pdf"
```

# Create synthesis plot

Create a plot with average effect sizes across the 3 main models :
EFturnover_WEIGHTED_mean, EFnestedness_WEIGHTED_mean, EFdistance.

Only biotic effects, because we add turnover and nestedness together,
and a comparison with LUI would not be fair (2 effects vs. 1 effect).

Average effect size : Depreciated, because it is almost exclusively
driven by EFdist model, because it is on a different scale. - either
scale and average (not done) - or take average rank (done)

```{r}
d1 <- fread(paste(pathtodata, "/analysis/output_datasets/all_LUI_mods_results_overview.csv", sep = ""))
d2 <- fread(paste(pathtodata, "/analysis/output_datasets/EFnestedness_turnover_WEIGTHED_mean_across_thresholds_maxsplines.csv", sep = ""))
mainmods <- merge(d1[, .(names, gdm_EFdistance_LUI)], d2, by = "names")
mainmods <- mainmods[type == "bio"]

###
# Sum to + nes and THEN rank
#
# sum turnover and nestedness of each model
mainmods <- data.table(stats::aggregate(mainmods[, .(gdm_EFdistance_LUI, EFturnover_WEIGTHED_mean_across_thresholds, EFnestedness_WEIGTHED_mean_across_thresholds)], 
                 by = list(mainmods$legendnames), 
                 FUN = sum))
setnames(mainmods, old = "Group.1", new = "legendnames")
#
# # calc average across main mods
# mainmods[, mean_across_main_mods := rowMeans(.SD), 
#          .SDcols = c("gdm_EFdistance_LUI", "EFturnover_WEIGTHED_mean_across_thresholds", "EFnestedness_WEIGTHED_mean_across_thresholds")]
#
# calc rank average across main mods
mainmods[, rank_EFdist := rank(gdm_EFdistance_LUI)]
mainmods[, rank_EFnes := rank(EFnestedness_WEIGTHED_mean_across_thresholds)]
mainmods[, rank_EFto := rank(EFturnover_WEIGTHED_mean_across_thresholds)]
mainmods[, rank_avg := rowMeans(.SD), .SDcols = c("rank_EFdist", "rank_EFnes", "rank_EFto")]


# Plot
mainmods <- merge(mainmods, unique(nicenames[, .(lui_ground, ground, color, legendnames, type)]), by = "legendnames", all.x = T)
# sort by effect size
mainmods <- mainmods[order(-rank_avg),]
# order the levels in the desired order
mainmods[, legendnames := factor(legendnames, levels = rev(mainmods$legendnames))]
#
# plot
p <- ggplot(data = mainmods, aes(x = legendnames, y = rank_avg, fill = color)) +
  geom_bar(stat = "identity", color = "black") + 
  coord_flip() + ylim(c(0, 15)) +
  scale_fill_identity() +
  ylab("average ranks across main models") + xlab("")
p
# saved as : "GDM_multifun_thresholds_synthesis_biotic_ranks.pdf" A5 landscape


# ###
# # rank, average to + nes and then average across models
# #
# # calc rank within main mods
# mainmods[, rank_EFdist := rank(gdm_EFdistance_LUI)]
# mainmods[, rank_EFnes := rank(EFnestedness_WEIGTHED_mean_across_thresholds)]
# mainmods[, rank_EFto := rank(EFturnover_WEIGTHED_mean_across_thresholds)]
# #
# # average the rank of to and nes
# mainmods <- data.table(stats::aggregate(mainmods[, .(rank_EFdist, rank_EFnes, rank_EFto)], by = list(mainmods$legendnames), FUN = mean))
# setnames(mainmods, old = "Group.1", new = "legendnames")
# #
# # average the rank across models
# mainmods[, rank_avg := rowMeans(.SD), .SDcols = c("rank_EFdist", "rank_EFnes", "rank_EFto")]
# #
```







# DEPRECIATED Calculate unweighted average models
newer code

```{r}
# select relevant columns from isplines all models
names <- model_names[model_class %in% c("multifun") & lui == "LUI", modelname]
names <- sort(names[grep("distance", names, invert = T)]) # exclude EFdist from the selection
names <- c("predictor", "Xpredictor", names)
isplines_all_models[, ..names]

# # #
# Calculate average over all tresholds EFturnover
# the resulting column has the identical number of rows than isplines_all_models
turnover_columns <- grep("turnover", names, value = T)
turnover_mean <- rowMeans(isplines_all_models[, ..turnover_columns])
isplines_all_models[, turnover_mean := turnover_mean]
rm(turnover_mean, turnover_columns)
#
# # Plot (shitty plot, just to see if values are +- possible)
# ggplot(isplines_all_models[predictor %in% grep("beta", isplines_all_models$predictor, value = T), ],
#        aes(x = Xpredictor, y = turnover_mean, col = predictor)) +
#   geom_line() +
#   theme(legend.position = "none")
# 
# ggplot(isplines_all_models[!predictor %in% grep("beta", isplines_all_models$predictor, value = T), ], aes(x = Xpredictor, y = turnover_mean)) +
#   geom_line() +
#   theme(legend.position = "none") +
#   facet_grid(cols = vars(predictor), scales = "free")


# # #
# Calculate average over all tresholds EFturnover
#
nestedness_columns <- grep("nestedness", names, value = T)
nestedness_mean <- rowMeans(isplines_all_models[, ..nestedness_columns])
isplines_all_models[, nestedness_mean := nestedness_mean]
#TODO
isplines_all_models[, nestedness_sd := lapply(.SD, sd), .SDcols = nestedness_columns]

rm(nestedness_mean, nestedness_columns)
# # Plot (shitty plot, just to see if values are +- possible)
# ggplot(isplines_all_models[predictor %in% grep("beta", isplines_all_models$predictor, value = T), ],
#        aes(x = Xpredictor, y = nestedness_mean, col = predictor)) +
#   geom_line() +
#   theme(legend.position = "none")
# 
# ggplot(isplines_all_models[!predictor %in% grep("beta", isplines_all_models$predictor, value = T), ], aes(x = Xpredictor, y = nestedness_mean)) +
#   geom_line() +
#   theme(legend.position = "none") +
#   facet_grid(cols = vars(predictor), scales = "free")
```
