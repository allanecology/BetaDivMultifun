---
title: "Functions Processing for BetaDivMultifun"
output:
  pdf_document: default
  html_notebook: default
---
------------------------------------------------------------- 
TODO : where is this done? 

check distributions
normality?

log transform non-normally distributed ones

check distributions again
-------------------------------------------------------------

This Vignette relies on the BExIS grassland functions dataset.
TODO : add information on where to get it/ ask for access

It also relies on a soil information dataset, which is prepared by `prepare_covariates.Rmd`.

# Data
The dataset used for gap filling is larger than the dataset used for the analysis. More variables enhance the performance of gapfilling.
```{r}
#USER : insert path to grassland function file and read in as grlfuns (uncomment line below)
# grlfuns <- data.table::fread("<pathto_grl_functions.csv>", header=T)
```
Grassland functions data used for gapfilling is stored in `raw_grlfuns`, selected variables according to info data.








# Gap filling
A Gap-filling of the functions dataset is performed, because many values are missing (NA). To enhance the predictions of the gapfilling, soil data is included. This introduces a bias, but reduces the imputation error. 

TODO : It can be commented out. (search for "uncomment if not wanted")

TODO : Two variables are produced, impdat which contains the gapfilled dataset which will be used in further analysis as well as imp_err, which contains the imputation error automatically estimated by missForest.


## Data preparation
Soil predictor data and Region is included to improve Gap filling. uncomment if not wanted!
```{r}
#USER : uncomment this block if soil information should not be included.
remove <- unique(glsoil$Soil.type)
glsoil[, (remove) := NULL]
glsoil[, Soil.type := as.factor(Soil.type)] # soil type as factor
rm(remove)
```

```{r}
befun <- merge(raw_grlfuns, glsoil, by="Plotn")
rm(glsoil)
befun[, Explo := as.factor(Explo)] # Region as a factor
#TODO : need to have parasitoid trap and all other columns which are "int" now as numeric?
```
cleaning out variables with too many missing values
```{r}
visdat::vis_miss(befun, sort_miss = T)
```
Values with over 21% missing data are excluded: mAMF hyphae, Parasitoid traps, Aggregation and pathogen infection.
```{r}
#USER : set treshold for missing values
treshold <- 0.21
t <- apply(befun, 2, function(x) sum(is.na(x)))
exclude <- names(which(t > 150 * treshold))
befun <- befun[, !colnames(befun) %in% exclude, with=F]
rm(t); rm(exclude); rm(treshold)
```


## Check Correlations
Of the non-factor columns only
```{r}
corrbefun <- befun[, !colnames(befun) %in% c("Explo", "Soil.type", "Plotn"), with=F]
M <- cor(corrbefun, use="pairwise.complete.obs")

# raw_grlfun_corrplot1
corrplot::corrplot(M,type="lower",addCoef.col = "black",method="color",diag=F, tl.srt=1, tl.col="black", mar=c(0,0,0,0), number.cex=0.6)

# raw_grlfun_corrplot2
corrplot::corrplot(M, type = "upper", tl.col="black", tl.srt=40, diag = F)

# only show high correlations
tres <- 0.7
M[which(M < tres & M > -tres)] <- 0
corrplot::corrplot(M, type = "upper", tl.col="black", tl.srt=40, diag = F)
```
Network
```{r}
# show network - is there a variable "alone"? bzw. isolated?
network <- igraph::graph_from_adjacency_matrix(M, weighted=T, mode="undirected", diag=F)
plot(network)
```
There is no variable which is not connected with at least one line to another variable. Lines represent correlations > threshold.
```{r}
rm(M)
```


## Log transformation
before changing values, store the original dataset
```{r}
befunbackup <- data.table::copy(befun)
```

remove negative values, remember to add again after imputation!

find negative values
```{r}
par(mar = c(10, 1, 3, 1))
boxplot(corrbefun, las=2)
# minimum of the numeric columns
negative <- colnames(corrbefun)[which(corrbefun[, lapply(.SD, function(x) min(x, na.rm=T))] < 0)]
```
remove negative values - shift whole vector up.
```{r}
# DEA
mindea <- min(befun$DEA, na.rm=T)
befun$DEA <- befun$DEA + abs(mindea)
# NO3.2014
minno <- min(befun$NO3.2014, na.rm=T)
befun$NO3.2014 <- befun$NO3.2014 + abs(minno)
# dung.decomposition
mindung <- min(befun$dung.decomposition, na.rm=T)
befun$dung.decomposition <- befun$dung.decomposition + abs(mindung)
```
Log transformation of the numeric columns. The natural logartithm of values smaller than 1 is negative. Tehrefore, all values are shifted by 1 again to avoid any negative values in the log transformed dataset.
```{r}
numcols <- colnames(befun)[!colnames(befun) %in% c("Explo", "Soil.type", "Plotn")]
befun[, (numcols) := lapply(.SD, as.numeric), .SDcols = numcols]
# log transform the values + 1, later shift them back.
befun[, (numcols) := lapply(.SD, function(x) log(x+1)), .SDcols = numcols]
```

## Imputation
The imputation is repeated 50 times, and the mean of the imputed values is taken. 
```{r}
# record NA value positions
naloc <- is.na(befun)
#TODO : maybe delete

# 50 imputations
impvals <- list()
imperr <- list()

for(i in 1:50){
  current <- missForest::missForest(befun[, !colnames(befun) %in% "Plotn", with=F])
  imperr[[i]] <- current$OOBerror
  current <- current$ximp
  # re-transform the numeric variables
  current[, (numcols) := lapply(.SD, function(x) (exp(x)-1)), .SDcols = numcols]
  # re-transform the negative values
  current$DEA <- current$DEA - abs(mindea)
  current$NO3.2014 <- current$NO3.2014 - abs(minno)
  current$dung.decomposition <- current$dung.decomposition - abs(mindung)
  # add back the "Plotn" column which was taken out for imputation
  current[, Plotn := befun$Plotn]
  # convert imputed data.table to matrix, as more handy for imputed values handling
  impvals[[i]] <- current
}
rm(current); rm(i)
# backup <- impvals
X <- do.call(rbind, impvals)
X[, (numcols) := lapply(.SD, as.numeric), .SDcols=numcols]
# backup2 <- data.table::copy(X)

# two ways of aggregating the 50 imputed values together
# Y <- aggregate(X[, ..numcols], list(Plotn=X$Plotn), mean)
Y <- X[, lapply(.SD, mean, na.rm=T), .SDcols=numcols, by=Plotn]
Y <- data.table::data.table(Y)
```
Compare imputed values with real values.
```{r}
# befunbackup is before imputation, Y is after imputation
plot(rep(1, 150), Y[, get(numcols[i])], col="red") ; points(rep(1, 150), befunbackup[, get(numcols[i])])

par(mfrow = c(2,7))
for(i in 1:length(numcols)){plot(rep(1, 150), Y[, get(numcols[i])], col="red", main=numcols[i]) ; points(rep(1, 150), befun[, get(numcols[i])])}
```

Note : still small problem : Clay in Y is scaled with scale01, but no in befun and befunbackup!
how come?
glsoil from prepare_covariates comes scaled with scale01.
where the fuck did I get the original values in the imputed set??








control : visualise the mean imputations
```{r, eval=F}
#TODO maybe take out, because this was used in the error prone way of aggregation
par(mfrow = c(2,7))
for(i in 1:12){plot(rep(1, 51), impvalsmat[i,]) ; points(1, impvalsmat$means[i], col = "red", pch=19)}
for(i in 13:25){plot(rep(1, 51), impvalsmat[i,]) ; points(1, impvalsmat$means[i], col = "red", pch=19)}
for(i in 26:38){plot(rep(1, 51), impvalsmat[i,]) ; points(1, impvalsmat$means[i], col = "red", pch=19)}
for(i in 39:51){plot(rep(1, 51), impvalsmat[i,]) ; points(1, impvalsmat$means[i], col = "red", pch=19)}
```
replace the NA values with the imputed values in the original dataset
```{r}
#TODO will take this out as well...
befun <- as.matrix(befun)
# befun[naloc]
befun[naloc] <- impvalsmat$means
```
control : visualise the imputed values
```{r}
#TODO : there is an ugly error somewhere with the values!!

par(mfrow = c(2,7))
for(i in 1:12){plot(rep(1, 51), befun[,i]) ; points(rep(1, length(befun[,i][naloc[,i]])), befun[,i][naloc[,i]], col = "red", pch=19)}
for(i in 13:25){plot(rep(1, 51), impvalsmat[i,]) ; points(1, impvalsmat$means[i], col = "red", pch=19)}
for(i in 26:38){plot(rep(1, 51), impvalsmat[i,]) ; points(1, impvalsmat$means[i], col = "red", pch=19)}
for(i in 39:51){plot(rep(1, 51), impvalsmat[i,]) ; points(1, impvalsmat$means[i], col = "red", pch=19)}
```




old stuff
```{r}


####################
# 5. IMPUTATION
# Impute
imputeall<-missForest(befun[,-1])
imp_error <- imputeall$OOBerror

#extract imputed data and retransform everything
impdat <- data.table(Plot0=befun$Plot, exp(imputeall$ximp[,-35])-1, Exploratory=imputeall$ximp[,35])
impdat$pathogen.infection <- impdat$pathogen.infection-abs(minpath)
impdat$forage.quality <- impdat$forage.quality-abs(minfor)

# exclude all columns that are not functions : see script "calculate_multifunctionality_dissimilarity_DEMO.r

#################
# 6. CLEANING
#################
# deleting all stored variables which I will not use any more in the analysis.
rm(befun) ; rm(M) ; rm(imputeall)  ; rm(funs) ; rm(minpath) ; rm(minfor)



# ##########################################
# # 7. DIFFERENT WAY OF ERROR ESTIMATION
# # Check if missForest is doing a good job
# # read functions table
# # befun is the table I use.
# 
# # calculate  % of missing data in each function (i.e. column)
# NA_per_column <- apply(befun,2,function(c) sum(is.na(c))/length(c))
# # remove NA's from the table (rows) - so you have a dataset with no NAs
# 
# navals <- apply(befun, 1, function(r) any(is.na(r)))
# noNA_befun <- befun[!navals,] ; rm(navals)
# original_noNA_befun <- noNA_befun
# 
# # randomly remove the same % in each column (as we had in the real dataset)
# settoNA <- round(NA_per_column*nrow(noNA_befun)) ; rm(NA_per_column) # the amount of values to set to NA
# whichrowsNA <- list()
# for(i in 1:length(settoNA)){
#   rows <- sample(1:nrow(noNA_befun), settoNA[[i]], replace=F) # get random rows to set to NA
#   noNA_befun[rows,i] <- NA
#   # save which rows were set to NA where
#   whichrowsNA[[i]] <- rows
# }
# rm(i) ; rm(rows) ; rm(settoNA)
# 
# # impute with missForest
# imputeall<-missForest(noNA_befun[,-1])
# imputeall$OOBerror # 12% of estimated error
# 
# check_imp <- as.data.frame(imputeall$ximp)
# 
# # compare imputed with real data (calculate NRMSE=normalized root mean square error value)
# nrmse(imputeall$ximp, noNA_befun[,-1], original_noNA_befun[,-1])
#
# rm(noNA_befun)
# ##########
# # visualization
# original_noNA_befun <- as.data.frame(original_noNA_befun)
# imp_no <- c(check_imp[whichrowsNA[[22]],22],check_imp[whichrowsNA[[18]],18], check_imp[whichrowsNA[[17]],17],
#             check_imp[whichrowsNA[[16]],16] , check_imp[whichrowsNA[[19]],19])
# rm(check_imp)
# true_no <- c(original_noNA_befun[whichrowsNA[[22]],22],original_noNA_befun[whichrowsNA[[18]],18],original_noNA_befun[whichrowsNA[[17]],17],
#              original_noNA_befun[whichrowsNA[[16]],16], original_noNA_befun[whichrowsNA[[19]],19])
# rm(original_noNA_befun) ; rm(whichrowsNA)
# imp_true_no <- cbind(imp_no,true_no, c(rep(22,11),rep(18,8),rep(17,8),rep(16,6), rep(19,7))) ; rm(imp_no) ; rm(true_no)
# plot(imp_true_no[,1],imp_true_no[,2] ,main="imputed versus true", col=imp_true_no[,3],pch=19,
#      xlab="imputed", ylab="true")
# rm(imp_true_no)
# abline(a=0, b=1,col="grey")
# # without standardization, there is a clustering (because of the different range the values are in)
# # with standardization, the clusters disappear.

```


