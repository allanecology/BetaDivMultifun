---
title: "cluster R"
author: "Noelle Schenk"
date: "July 10, 2019"
output: html_document
---
# Cluster
In order to run R code on the cluster, please follow the instructions on the [UBELIX help page](https://hpc-unibe-ch.github.io//). There is a great documentation (also about using R on the cluster).
- get user profile
    - log in with `ssh <username>@submit.unibe.ch`
- install R and the required packages
- run the examples
```{bash}
module load vital-it
module avail 2>&1 | grep " R\/"
# check if your R version is there, e.g. 3.6.1 for me
module load R/3.6.1

# install packages
R # open R and within R, type : 
install.packages("gdm")
# accept using personal library
# select CRAN
```


# Data
model results need to be uploaded : <modelname>_input.Rds files which contian site-pair tables, as used for gdm models.
```{bash}
ssh <username>@submit.unibe.ch

scp /path/to/file <username>@submit.unibe.ch:path/to/target_dir/

scp <username>@submit.unibe.ch:path/to/file /path/to/target_dir/
```
source : https://docs.id.unibe.ch/ubelix/getting-started


# Scripts
2 scripts are required : an R script which contains the R code and a SLURM script which contains the cluster code.
- R script : `gdm.R`
- cluster script : `Rbatch.sh`

To run parallel, you need to give the number of cores in (1) the slurm script and (2) the R script. `Sys.getenv("SLURM_CPUS_PER_TASK")` can be used.

**gdm.R**
```{r}
name <- "gdm_Groundwater.recharge_LUI"
gdm_model <- readRDS(paste("data/", name, "_input.Rds", sep = ""))

# fullModelOnly = T : test only the full variable set, not model - 1 variable
# fullModelOnly = F : estimate model significance and variable importance/ significance using m...
# parallel = T
# cores : number of cores to be registered for parralel processing
# outFile : name of outfile

varimp_output <- gdm::gdm.varImp(gdm_model, geo=T, parallel=T, nPerm =100, cores = 10, fullModelOnly = T)

saveRDS(varimp_output, file = paste("out/", name, "_perm.Rds", sep = ""))
```


**Rbatch.sh**
```{bash}
#!/bin/sh
#SBATCH --mail-user=<mail>
#SBATCH --mail-type=end,fail

#SBATCH --mem-per-cpu=8G
#SBATCH --time=12:00:00
#SBATCH --cpus-per-task=10
#SBATCH --job-name="gdm_Groundwater.recharge_LUI"

# change
#SBATCH --output=gdm_Groundwater.recharge_LUI.out
#SBATCH --error=gdm_Groundwater.recharge_LUI.err

############################# execute code here################################ 
module load vital-it
module load R/latest

R CMD BATCH --no-save -no-restore gdm.R
```
Note : there is also the option `#SBATCH --workdir=.` which has been taken out, because it did not work. All relative paths of the script are relative to the location from where `sbatch` has been runned.

To submit the job to the cluster (run both scripts).
```{bash}
sbatch Rbatch.sh
squeue -u <username>
```


# workflow
cluster workflow 
1. change name in gdm.R to current .Rds file
2. change job name ,


3. sbatch Rbatch.sh
4. record job ID in this file

automatically replace names
```{bash}
sed -i 's/gdm_Root.decomposition_LUI/gdm_EFdistance_LUI/g' gdm.R Rbatch.sh
```


outfiles : 
- gdm_EFdistance_LUI_permutation.Rds : 4 permutations

value
- 1. table : summarizes full model deviance, percent deviance explained by full model
    p-value of the full model, number of permutations
- 2. table : variable importance
- 3. variable significance
- number of permutations used to calc stats for that model (some GDMs may fail to fit for some permutations/ variable combinations)

Significance is estimated using the bootstrapped p-value when the variable has been permuted.

```{r}
# show the p-values
gdmperm[[3]][,1]
# show the number of permutations used
gdmperm[[4]][,1]
```

